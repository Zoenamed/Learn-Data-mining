# 模型融合

模型融合这个步骤背后实际上是一种集成学习（Ensemble Learning）的思想
> 集成学习：通过构建多个学习器一起结合起来完成具体的学习任务。 

“如何将多个学习器结合起来？”这个问题就构成了模型融合的不同方法。

模型融合的主要方法：

1.简单加权融合：

2.stacking/blending

3.boosting/bagging

[集成学习的常用方法](https://www.cnblogs.com/csu-lmw/p/12110009.html)：
Boosting\Bagging\Voting/Averaging\Binning\Stacking\Blending

回归问题常采用融合方法，而分类问题可采用融合、投票

一、回归问题

1.简单加权融合：

- 简单加权平均：
给每个模型的预测值赋予相应权重并相加得到结果
- mean平均：
直接取所有模型预测值的平均
- median平均：取所有预测结果的中位数

2.Stacking融合：
> from sklearn.model_selection import StratifiedKFold

分层：
1. 将数据集分割为两份，分别用不同的模型进行训练
2. 引入一个较为简单的模型作为第二层， 用真实值fit（拟合）模型，再用1中预测结果来predict（预测）得最终预测值。

二、分类问题：
1. Voting（投票）
> sklearn中的VotingClassifier
>
> 原理：少数服从多数

- 硬投票：直接对多个模型进行投票，每个模型权重均相同，最终结果为票数最多的模型
- 软投票：比起硬投票增加了设置权重的功能

2. Stacking\Blending的融合

Blending:
将划分一定比例的训练集作为新的训练集，剩余部分作为测试集

分层：
1. 在新训练集上训练多个模型，去预测测试集、新测试集
2.  用新测试集在1的预测结果作为新特征训练第二次.用原预测集在1的预测结果作为特征，用2的模型预测得最终结果

3.其他方法：

Stacking变换：将特征放进模型中预测，将预测结果变换并作为新的特征加入原有特征中再进行模型预测得结果。

4.一般比赛中效果较显著的方法
xgboost
lgb

近期了解了AdaBoost的发展始末，了解了Boost方法都逃不开的基本思想：
> “对于一个复杂任务来说，将多个专家的判断进行适当的综合所得出的判断，要比其中任何一个专家单独判断的好。实际上，就是‘三个臭皮匠顶个诸葛亮’的道理。”——《统计学习方法（李航）》

而有趣的是Boost方法的提出来源于一个重要的问题，Kearns和Valiant提出了“strongly learnable” 和“weakly learnable”的概念。在后来Schapire居然证明了“强可学习与弱可学习是等价的”这么一个有违常理的结论，从而促使Boost方法的形成。